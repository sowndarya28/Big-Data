{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58901a47-4bce-4685-9d9d-5c022517322b",
   "metadata": {},
   "source": [
    ". Write a Python program to read a Hadoop configuration file and display the core components of Hadoop.\n",
    "2. Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory.\n",
    "3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach.\n",
    "4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API.\n",
    "5. Develop a Python program that lists all the files and directories in a specific HDFS path.\n",
    "6. Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities.\n",
    "7. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output.\n",
    "8. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution.\n",
    "9. Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing the impact on overall job execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228068e9-861b-4ecc-8581-390fac6224db",
   "metadata": {},
   "source": [
    "Python program to read a Hadoop configuration file and display the core components of Hadoop:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387fc072-42d9-4f81-be31-c112f7b37637",
   "metadata": {},
   "source": [
    "def read_hadoop_config(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    core_components = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.startswith('dfs.namenode') or line.startswith('dfs.datanode'):\n",
    "            component = line.split('=')[0].strip()\n",
    "            core_components.append(component)\n",
    "    \n",
    "    return core_components\n",
    "\n",
    "# Example usage\n",
    "config_file_path = 'hadoop.conf'\n",
    "components = read_hadoop_config(config_file_path)\n",
    "print(\"Core components of Hadoop:\")\n",
    "for component in components:\n",
    "    print(component)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918894be-937c-4961-a0a3-3aa467d00e01",
   "metadata": {},
   "source": [
    "Python function to calculate the total file size in a Hadoop Distributed File System (HDFS) directory:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9886dc6-925b-4671-b4db-ecdf7ba06e38",
   "metadata": {},
   "source": [
    "import subprocess\n",
    "\n",
    "def get_directory_size(directory_path):\n",
    "    command = f'hdfs dfs -du -s {directory_path}'\n",
    "    output = subprocess.check_output(command, shell=True).decode('utf-8')\n",
    "    size = int(output.split()[0])\n",
    "    return size\n",
    "\n",
    "# Example usage\n",
    "directory_path = '/user/myuser/data'\n",
    "total_size = get_directory_size(directory_path)\n",
    "print(f\"Total size of {directory_path}: {total_size} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7848b2-66c9-4b72-98eb-2694e65b2c3d",
   "metadata": {},
   "source": [
    "Python program to extract and display the top N most frequent words from a large text file using the MapReduce approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba14871-7a1b-4a5a-a305-c8d98c22af47",
   "metadata": {},
   "source": [
    "from collections import Counter\n",
    "import subprocess\n",
    "\n",
    "def get_top_words(file_path, n):\n",
    "    command = f'hadoop jar wordcount.jar WordCount {file_path} output'\n",
    "    subprocess.call(command, shell=True)\n",
    "\n",
    "    top_words = []\n",
    "    with open('output/part-r-00000', 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        word_counts = [line.strip().split('\\t') for line in lines]\n",
    "        word_counts.sort(key=lambda x: int(x[1]), reverse=True)\n",
    "        top_words = [word for word, count in word_counts[:n]]\n",
    "    \n",
    "    return top_words\n",
    "\n",
    "# Example usage\n",
    "file_path = '/user/myuser/large_file.txt'\n",
    "top_n = 10\n",
    "top_words = get_top_words(file_path, top_n)\n",
    "print(f\"Top {top_n} frequent words:\")\n",
    "for word in top_words:\n",
    "    print(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cfab63-9ad3-4e50-a773-3fdbd2a57d97",
   "metadata": {},
   "source": [
    "Python script to check the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0b1c42-1845-4d45-b2f3-e2940257b0ee",
   "metadata": {},
   "source": [
    "import requests\n",
    "\n",
    "def check_cluster_health():\n",
    "    namenode_url = 'http://namenode:50070/jmx?qry=Hadoop:service=NameNode,name=NameNodeInfo'\n",
    "    datanode_url = 'http://datanode:50075/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo'\n",
    "    \n",
    "    namenode_health = requests.get(namenode_url).json()['beans'][0]['State']\n",
    "    datanode_health = requests.get(datanode_url).json()['beans'][0]['State']\n",
    "    \n",
    "    return namenode_health, datanode_health\n",
    "\n",
    "# Example usage\n",
    "nn_health, dn_health = check_cluster_health()\n",
    "print(f\"NameNode health: {nn_health}\")\n",
    "print(f\"DataNode health: {dn_health}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb421b60-b138-45c9-b139-e3cf339eff52",
   "metadata": {},
   "source": [
    "Python program to list all the files and directories in a specific HDFS path:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892783b3-caa0-4ac1-a579-b379202ceff0",
   "metadata": {},
   "source": [
    "import subprocess\n",
    "\n",
    "def list_hdfs_path(path):\n",
    "    command = f'hdfs dfs -ls {path}'\n",
    "    output = subprocess.check_output(command, shell=True).decode('utf-8')\n",
    "    lines = output.strip().split('\\n')[1:]\n",
    "    \n",
    "    files = []\n",
    "    directories = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line_parts = line.split()\n",
    "        file_type = line_parts[0][0]\n",
    "        file_name = line_parts[-1]\n",
    "        \n",
    "        if file_type == 'd':\n",
    "            directories.append(file_name)\n",
    "        else:\n",
    "            files.append(file_name)\n",
    "    \n",
    "    return files, directories\n",
    "\n",
    "# Example usage\n",
    "hdfs_path = '/user/myuser/data'\n",
    "files, directories = list_hdfs_path(hdfs_path)\n",
    "\n",
    "print(\"Files:\")\n",
    "for file in files:\n",
    "    print(file)\n",
    "\n",
    "print(\"\\nDirectories:\")\n",
    "for directory in directories:\n",
    "    print(directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19f59a0-2f5d-4840-b4a1-39aa75914ebf",
   "metadata": {},
   "source": [
    "Python program to analyze the storage utilization of DataNodes in a Hadoop cluster and identify the nodes with the highest and lowest storage capacities:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2dcb4d-fbcd-4cc5-8469-a61847826a82",
   "metadata": {},
   "source": [
    "import requests\n",
    "\n",
    "def analyze_datanode_storage():\n",
    "    datanode_url = 'http://datanode:50075/jmx?qry=Hadoop:service=DataNode,name=FSDatasetState'\n",
    "    \n",
    "    response = requests.get(datanode_url).json()\n",
    "    datanodes = response['beans'][0]['VolumeInfo']\n",
    "    \n",
    "    storage_utilization = {}\n",
    "    \n",
    "    for datanode in datanodes:\n",
    "        node_name = datanode['key'].split('=')[1]\n",
    "        used = datanode['usedSpace']\n",
    "        capacity = datanode['capacity']\n",
    "        utilization = (used / capacity) * 100\n",
    "        storage_utilization[node_name] = utilization\n",
    "    \n",
    "    max_utilization_node = max(storage_utilization, key=storage_utilization.get)\n",
    "    min_utilization_node = min(storage_utilization, key=storage_utilization.get)\n",
    "    \n",
    "    return max_utilization_node, min_utilization_node\n",
    "\n",
    "# Example usage\n",
    "max_node, min_node = analyze_datanode_storage()\n",
    "print(f\"DataNode with highest storage utilization: {max_node}\")\n",
    "print(f\"DataNode with lowest storage utilization: {min_node}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566c5efa-ef4b-4a30-910a-9c21460b3a80",
   "metadata": {},
   "source": [
    "Python script to interact with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92229825-dd97-46a2-be4d-bb57ef28325e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
