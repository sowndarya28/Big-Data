{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0cc793c-0639-4d10-b346-f31cf87b31e1",
   "metadata": {},
   "source": [
    "1. Working with RDDs:\n",
    "   a) Write a Python program to create an RDD from a local data source.\n",
    "   b) Implement transformations and actions on the RDD to perform data processing tasks.\n",
    "   c) Analyze and manipulate data using RDD operations such as map, filter, reduce, or aggregate.\n",
    "\n",
    "2. Spark DataFrame Operations:\n",
    "   a) Write a Python program to load a CSV file into a Spark DataFrame.\n",
    "   b)Perform common DataFrame operations such as filtering, grouping, or joining.\n",
    "   c) Apply Spark SQL queries on the DataFrame to extract insights from the data.\n",
    "\n",
    "3. Spark Streaming:\n",
    "  a) Write a Python program to create a Spark Streaming application.\n",
    "   b) Configure the application to consume data from a streaming source (e.g., Kafka or a socket).\n",
    "   c) Implement streaming transformations and actions to process and analyze the incoming data stream.\n",
    "\n",
    "4. Spark SQL and Data Source Integration:\n",
    "   a) Write a Python program to connect Spark with a relational database (e.g., MySQL, PostgreSQL).\n",
    "   b)Perform SQL operations on the data stored in the database using Spark SQL.\n",
    "   c) Explore the integration capabilities of Spark with other data sources, such as Hadoop Distributed File System (HDFS) or Amazon S3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242e1ad6-cbe4-43c7-b7a3-9678e32cbbf0",
   "metadata": {},
   "source": [
    "Working with RDDs:\n",
    "a) Here's an example of creating an RDD from a local data source:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b204e0e-1a3d-485c-9897-edfe8d525cbe",
   "metadata": {},
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"RDD Example\")\n",
    "\n",
    "# Create an RDD from a local data source (list)\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Print the RDD elements\n",
    "print(rdd.collect())\n",
    "\n",
    "# Close the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddbf367-acfe-4ffe-8f02-e2017120fd67",
   "metadata": {},
   "source": [
    "b) Implementing transformations and actions on the RDD:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb26d3b-9ea7-468f-b448-09fa98f99dc4",
   "metadata": {},
   "source": [
    "# Transformations\n",
    "rdd = rdd.map(lambda x: x * 2)  # Multiply each element by 2\n",
    "\n",
    "# Actions\n",
    "total = rdd.reduce(lambda x, y: x + y)  # Calculate the sum of all elements\n",
    "\n",
    "# Print the transformed RDD and the result of the action\n",
    "print(rdd.collect())\n",
    "print(total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643e8248-9df7-44be-883f-5defdc624df2",
   "metadata": {},
   "source": [
    " Analyzing and manipulating data using RDD operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6267f9e5-7c03-4125-9da5-a9509e78cf78",
   "metadata": {},
   "source": [
    "# Filter RDD elements\n",
    "filtered_rdd = rdd.filter(lambda x: x % 2 == 0)  # Keep only even numbers\n",
    "\n",
    "# Aggregate RDD elements\n",
    "product = rdd.aggregate(1, lambda acc, x: acc * x, lambda acc1, acc2: acc1 * acc2)  # Calculate the product of all elements\n",
    "\n",
    "# Print the filtered RDD and the result of the aggregation\n",
    "print(filtered_rdd.collect())\n",
    "print(product)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de7462e-bd0b-4e2d-a14d-fed4cc21af6f",
   "metadata": {},
   "source": [
    "Spark DataFrame Operations:\n",
    "a) Loading a CSV file into a Spark DataFrame:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf545496-0eae-4963-82e6-eec2718f70b0",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrame Example\").getOrCreate()\n",
    "\n",
    "# Load a CSV file into a DataFrame\n",
    "df = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Show the DataFrame schema and some sample data\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858cf8e6-253d-4712-8d5b-12bdee983a44",
   "metadata": {},
   "source": [
    "b) Performing common DataFrame operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aee7f7-d894-401a-bc07-bb409191ce76",
   "metadata": {},
   "source": [
    "# Filter DataFrame rows\n",
    "filtered_df = df.filter(df[\"column\"] > 10)  # Keep only rows where \"column\" is greater than 10\n",
    "\n",
    "# Group DataFrame by a column and calculate aggregations\n",
    "grouped_df = df.groupBy(\"column\").agg({\"other_column\": \"sum\"})  # Group by \"column\" and calculate the sum of \"other_column\"\n",
    "\n",
    "# Join two DataFrames\n",
    "joined_df = df1.join(df2, on=\"common_column\", how=\"inner\")  # Inner join df1 and df2 on the common_column\n",
    "\n",
    "# Show the results of the operations\n",
    "filtered_df.show()\n",
    "grouped_df.show()\n",
    "joined_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3341bd35-6cec-49cf-8d4a-1de27b173ab0",
   "metadata": {},
   "source": [
    "c) Applying Spark SQL queries on the DataFrame:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5380fce5-2ca1-4d70-b5d1-5a770be8ea13",
   "metadata": {},
   "source": [
    "# Register the DataFrame as a temporary table\n",
    "df.createOrReplaceTempView(\"table_name\")\n",
    "\n",
    "# Run SQL queries on the DataFrame\n",
    "result = spark.sql(\"SELECT * FROM table_name WHERE column > 10\")\n",
    "\n",
    "# Show the result of the query\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a54e14-0a02-46ce-8ae1-61a06ee94dd0",
   "metadata": {},
   "source": [
    "Spark Streaming:\n",
    "a) Creating a Spark Streaming application:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71df0563-f12e-430c-a15d-8bea8382e119",
   "metadata": {},
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a StreamingContext\n",
    "sc = SparkContext(\"local[2]\", \"Streaming Example\")\n",
    "ssc = StreamingContext(sc, batchDuration=1)\n",
    "\n",
    "# Create a DStream from a streaming source (e.g., socket)\n",
    "dstream = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# Perform transformations and actions on the DStream\n",
    "processed_stream = dstream.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Print the processed stream\n",
    "processed_stream.pprint()\n",
    "\n",
    "# Start the streaming context\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9a1055-b7e5-4736-8dfb-b099a3d3e12b",
   "metadata": {},
   "source": [
    "c) Implementing streaming transformations and actions:\n",
    "\n",
    "The flatMap, map, and reduceByKey operations in the above example are examples of transformations and actions applied to the DStream. You can use various other transformations and actions provided by Spark Streaming to process and analyze the incoming data stream.\n",
    "\n",
    "Spark SQL and Data Source Integration:\n",
    "a) Connecting Spark with a relational database:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cdfe24-d6d4-4ac7-a79d-caed70bebe10",
   "metadata": {},
   "source": [
    "c) Implementing streaming transformations and actions:\n",
    "\n",
    "The flatMap, map, and reduceByKey operations in the above example are examples of transformations and actions applied to the DStream. You can use various other transformations and actions provided by Spark Streaming to process and analyze the incoming data stream.\n",
    "\n",
    "Spark SQL and Data Source Integration:\n",
    "a) Connecting Spark with a relational database:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71dcbaf-b14a-4680-9b89-2fb20cd96f54",
   "metadata": {},
   "source": [
    "b) Performing SQL operations on the data stored in the database:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0d51c0-66ee-4437-b584-4d21d9bb83c8",
   "metadata": {},
   "source": [
    "# Register the DataFrame as a temporary table\n",
    "df.createOrReplaceTempView(\"table_name\")\n",
    "\n",
    "# Run SQL queries on the DataFrame\n",
    "result = spark.sql(\"SELECT * FROM table_name WHERE column > 10\")\n",
    "\n",
    "# Show the result of the query\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd255e0-fe14-4d4e-9cde-b1af51e60da5",
   "metadata": {},
   "source": [
    "c) Exploring integration capabilities with other data sources:\n",
    "\n",
    "Spark provides built-in support for various data sources, including HDFS and Amazon S3. You can load data from and save data to these sources using the appropriate methods provided by Spark:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c3cbbe-9475-4192-9e17-4af011669df2",
   "metadata": {},
   "source": [
    "# Load data from HDFS into a DataFrame\n",
    "df_hdfs = spark.read.csv(\"hdfs://localhost:9000/path/to/file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Save DataFrame to HDFS\n",
    "df_hdfs.write.csv(\"hdfs://localhost:9000/path/to/save.csv\")\n",
    "\n",
    "# Load data from Amazon S3 into a DataFrame\n",
    "df_s3 = spark.read.csv(\"s3a://bucket_name/path/to/file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Save DataFrame to Amazon S3\n",
    "df_s3.write.csv(\"s3a://bucket_name/path/to/save.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a286b94-30fc-43e6-85ee-fe6783091258",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
